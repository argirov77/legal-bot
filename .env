CHROMA_PERSIST_DIR=/chroma_db
VECTOR_STORE=mock

# Embeddings
EMBEDDING_MODEL_PATH=/models/all-MiniLM-L6-v2
EMBEDDING_DEVICE=cpu

# LLM
LLM_PROVIDER=transformers
LLM_MODEL_PATH=/models/bgpt-7b
# Для обратной совместимости — провайдер возьмёт первый доступный:
# LLM_BG1_PATH=/models/bgpt-7b
# LLM_BG2_PATH=/models/gemma-2-bg
LLM_DEVICE=cuda
LLM_DEVICE_MAP=single
LLM_TORCH_DTYPE=auto
LLM_QUANT=
LLM_MAX_TOKENS=256
LLM_TEMPERATURE=0.0
LLM_STUB=false

OCR_LANG=bul
INSTALL_HEAVY=true
# Для Docker-образа подтягивать CUDA-колёса torch
USE_CUDA=true
FORCE_LOAD_ON_START=false

# Кэш моделей (ускоряет и в Docker, и локально)
HF_HOME=/models
